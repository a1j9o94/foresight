# Foresight Demo Configuration
# See demo/DESIGN.md for full documentation

# Backend (FastAPI) settings
backend:
  host: "0.0.0.0"
  port: 8000
  debug: false
  cors_origins:
    - "http://localhost:3000"
    - "http://127.0.0.1:3000"

# Frontend (React/Vite) settings
frontend:
  port: 3000
  api_url: "http://localhost:8000"

model:
  # Vision-Language Model
  vlm: "Qwen/Qwen2.5-VL-7B-Instruct"
  vlm_dtype: "float16"

  # Video Decoder
  video_decoder: "Lightricks/LTX-Video"
  decoder_dtype: "float16"

  # P2 Hybrid Encoder (DINOv2 for spatial features)
  hybrid_encoder: true
  dino_model: "facebook/dinov2-vitl14"
  dino_dtype: "float16"

  # Adapter checkpoint (when available)
  adapter_path: null  # Path to trained adapter weights

generation:
  # Video generation settings
  num_frames: 30
  fps: 15
  resolution: [512, 512]
  guidance_scale: 7.5
  num_inference_steps: 25

  # Query tokens (GLP architecture)
  num_query_tokens: 32

ui:
  # Feature toggles
  show_metrics: true
  show_latents: false  # Enable for advanced/debug mode
  auto_play_predictions: true

  # Panel sizes (percentage)
  chat_panel_width: 60
  thoughts_panel_width: 40

performance:
  # Inference optimization
  batch_size: 1
  use_fp16: true
  use_flash_attention: true
  cache_encodings: true

  # Memory management
  offload_to_cpu: false
  sequential_offload: false

  # History limits
  max_history_messages: 50
  max_prediction_cache: 10

# Development/testing
dev:
  mock_mode: true  # Use mock responses (no GPU required)
  mock_latency_ms: 2000  # Simulated inference time
