# Template for experiment results
# Copy this to your experiment directory and fill in

experiment_id: "<experiment-id>"  # e.g., c1-vlm-latent-sufficiency
claim: "<claim or question being tested>"
status: in_progress  # not_started | in_progress | completed | blocked | failed | pivoted

# Execution metadata
executed_by: "<your-instance-id>"
started_at: "<ISO-8601 timestamp>"
completed_at: null  # Fill when done

# Pre-registered success criteria (copy from experiment plan)
success_criteria:
  metric_1_threshold: 0.0
  metric_2_threshold: 0.0

# Results from each sub-experiment
results:
  experiments:
    e1_experiment_name:
      status: completed  # completed | in_progress | skipped | failed
      finding: "Brief description of what we learned"
      metrics:
        metric_1: 0.0
        metric_2: 0.0
      artifacts:
        - artifacts/plot_1.png
        - artifacts/checkpoint.pt

    e2_experiment_name:
      status: in_progress
      finding: null
      metrics: {}
      artifacts: []

# Overall assessment (fill when all experiments complete)
assessment:
  success_criteria_met: false  # true | false

  # Report achieved values for each criterion
  metric_1_achieved: 0.0
  metric_2_achieved: 0.0

  confidence: medium  # high | medium | low
  confidence_notes: |
    Explain confidence level.
    Note any caveats or edge cases.

# Recommendation based on results
recommendation: investigate  # proceed | pivot | investigate | block
recommendation_notes: |
  Explain the recommendation.
  If pivot, which pivot option from the plan?

# Issues discovered
blockers: []  # Things that prevent progress
issues:  # Things that need attention but don't block
  - "Issue 1"
  - "Issue 2"

# Link to detailed analysis
detailed_analysis: analysis.ipynb
